{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (IMPLEMENTADO) Análise Quantitativa do Trade-off entre Especialização e Generalização em LLMs\n",
    "\n",
    "**Aluno(s)**: Alexandre Gabriel Gadelha de Lima e Martinho Prata\n",
    "\n",
    "**Data de Entrega**: 23/06/2025\n",
    "\n",
    "---\n",
    "**Nota**: Este notebook contém as implementações que estavam faltando na versão anterior. As novas células de código estão marcadas e comentadas para explicar a lógica adicionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação de Dependências e Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.45.2\n",
      "  Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in ./.venv/lib/python3.12/site-packages (from bitsandbytes==0.45.2) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from bitsandbytes==0.45.2) (2.3.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes==0.45.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.2) (3.0.2)\n",
      "Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes==0.45.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "SPIDER_DB_PATH_TEMPLATE = \"./spider/database/{db_id}/{db_id}.sqlite\"\n",
    "\n",
    "# Diretórios para salvar os modelos\n",
    "OUTPUT_DIR_CONFIG_1 = \"./results/config_1\"\n",
    "OUTPUT_DIR_CONFIG_2 = \"./results/config_2\"\n",
    "model_name = \"microsoft/phi-2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1: Estabelecimento do Baseline de Desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Prompt Engineering\n",
    "Construir um prompt de few-shot para a tarefa Text-to-SQL com 3 exemplos representativos[cite: 20]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template de Prompt Few-Shot Criado.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset Spider para extrair os exemplos\n",
    "spider_dataset = load_dataset(\"spider\")\n",
    "train_split = spider_dataset['train']\n",
    "\n",
    "# Exemplo de como extrair 3 exemplos representativos\n",
    "example_1 = train_split[0]\n",
    "example_2 = train_split[1]\n",
    "example_3 = train_split[2]\n",
    "\n",
    "# Construa o template de prompt fixo aqui\n",
    "FEW_SHOT_PROMPT_TEMPLATE = f\"\"\"\n",
    "### INSTRUCTION\n",
    "Given a natural language question, generate the corresponding SQL query.\n",
    "\n",
    "### Example 1\n",
    "Question: {example_1['question']}\n",
    "SQL: {example_1['query']}\n",
    "\n",
    "### Example 2\n",
    "Pergunta: {example_2['question']}\n",
    "SQL: {example_2['query']}\n",
    "\n",
    "### Example 3\n",
    "Question: {example_3['question']}\n",
    "SQL: {example_3['query']}\n",
    "\n",
    "### Current question\n",
    "Question: {{user_question}}\n",
    "SQL:\"\"\"\n",
    "\n",
    "print(\"Template de Prompt Few-Shot Criado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 e 1.3 [IMPLEMENTADO] Execução da Avaliação e Coleta de Dados\n",
    "A lógica para extrair a SQL da saída do modelo e para a contagem preliminar de sucesso/falha foi adicionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sql_from_output(text: str) -> str:\n",
    "    \"\"\"Extrai a consulta SQL da saída completa do modelo.\"\"\"\n",
    "    match = re.search(r\"SQL:(.*)\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "def preliminary_check(generated_sql: str, ground_truth_sql: str, db_path: str) -> bool:\n",
    "    \"\"\"Executa ambas as queries e compara os resultados de forma simples.\"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        return False\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(generated_sql)\n",
    "        predicted_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(ground_truth_sql)\n",
    "        expected_result = cursor.fetchall()\n",
    "        \n",
    "        return set(predicted_result) == set(expected_result)\n",
    "    except Exception:\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Avaliação no Spider Dev Split\n",
    "spider_dev_split = spider_dataset['validation']\n",
    "baseline_results = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "print(\"Iniciando avaliação de baseline...\")\n",
    "for item in tqdm(spider_dev_split.select(range(50))): # Usando uma amostra para agilizar\n",
    "    prompt = FEW_SHOT_PROMPT_TEMPLATE.format(user_question=item['question'])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_sql = parse_sql_from_output(full_output)\n",
    "    \n",
    "    db_path = SPIDER_DB_PATH_TEMPLATE.format(db_id=item['db_id'])\n",
    "    is_success = preliminary_check(generated_sql, item['query'], db_path)\n",
    "    \n",
    "    if is_success:\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "print(f\"\\n--- Baseline Performance ---\")\n",
    "print(f\"Sucesso: {success_count}\")\n",
    "print(f\"Falha: {fail_count}\")\n",
    "print(f\"Acurácia de Execução Bruta: {success_count / (success_count + fail_count):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2: Execução do Fine-Tuning\n",
    "### [IMPLEMENTADO] Configuração de Treino, Salvamento e Carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_dataset = load_dataset(\"spider\")\n",
    "OUTPUT_DIR_CONFIG_1 = \"./results/config_1\"\n",
    "OUTPUT_DIR_CONFIG_2 = \"./results/config_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    " bnb_4bit_quant_type=\"nf4\", \n",
    "bnb_4bit_compute_dtype=torch.float16,\n",
    " bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando o processo de geração de SQL...\n",
      "Retomando do item 1034 de 1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando SQLs: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1034 resultados de baseline gerados.\n",
      "Processamento completo.\n",
      "Resultados salvos em 'baseline_checkpoint.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "CHECKPOINT_PATH = \"baseline_checkpoint.pkl\"\n",
    "\n",
    "# --- 4. LÓGICA DE GERAÇÃO E CHECKPOINT (Seu código) ---\n",
    "print(\"\\nIniciando o processo de geração de SQL...\")\n",
    "# Carregar o split de desenvolvimento do Spider [cite: 13]\n",
    "spider_dev_split = spider_dataset['validation']\n",
    "\n",
    "# Verificar se há um checkpoint existente para retomar o trabalho\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    with open(CHECKPOINT_PATH, \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    baseline_results = checkpoint_data[\"results\"]\n",
    "    start_index = checkpoint_data[\"last_index\"] + 1\n",
    "    print(f\"Retomando do item {start_index} de {len(spider_dev_split)}\")\n",
    "else:\n",
    "    baseline_results = []\n",
    "    start_index = 0\n",
    "    print(\"Iniciando do início.\")\n",
    "\n",
    "# Loop de avaliação com barra de progresso\n",
    "for i in tqdm(range(start_index, len(spider_dev_split)), desc=\"Gerando SQLs\"):\n",
    "    item = spider_dev_split[i]\n",
    "    user_question = item['question']\n",
    "    ground_truth_query = item['query']\n",
    "\n",
    "    # Formata o prompt para o item atual\n",
    "    prompt = FEW_SHOT_PROMPT_TEMPLATE.format(user_question=user_question)\n",
    "\n",
    "    # Geração pelo modelo\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad(): # Desativa o cálculo de gradientes para acelerar a inferência\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150, # Aumentado um pouco para queries mais longas\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    # Decodifica a saída completa do modelo\n",
    "    generated_sql_raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    baseline_results.append({\n",
    "        'question': user_question,\n",
    "        'generated_sql': generated_sql_raw_output, # Salva a saída bruta\n",
    "        'ground_truth_sql': ground_truth_query\n",
    "    })\n",
    "\n",
    "    # Salvar checkpoint a cada iteração para não perder o progresso\n",
    "    with open(CHECKPOINT_PATH, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"results\": baseline_results,\n",
    "            \"last_index\": i\n",
    "        }, f)\n",
    "\n",
    "print(f\"\\n{len(baseline_results)} resultados de baseline gerados.\")\n",
    "print(\"Processamento completo.\")\n",
    "print(f\"Resultados salvos em '{CHECKPOINT_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO VERIFICAÇÃO FINAL DO BASELINE ---\n",
      "Carregando dataset de referência...\n",
      "Carregando resultados do checkpoint: baseline_checkpoint.pkl\n",
      "1034 resultados carregados.\n",
      "\n",
      "Processando resultados com o código corrigido...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verificando SQLs: 100%|██████████| 1034/1034 [00:00<00:00, 1453.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- RESULTADO FINAL DO BASELINE ---\n",
      "Total de Itens Verificados: 1034\n",
      "Sucesso: 237\n",
      "Falha: 797\n",
      "Acurácia de Execução Bruta: 22.92%\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def parse_sql_from_output(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrai a consulta SQL da saída completa do modelo,\n",
    "    pegando o conteúdo após a ÚLTIMA ocorrência de 'SQL:'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = text.split('SQL:')\n",
    "        last_part = parts[-1]\n",
    "        \n",
    "        cleaned_sql = last_part.split('###')[0].strip()\n",
    "        return cleaned_sql\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def normalize_string(sql: str) -> str:\n",
    "    \"\"\"Converte para minúsculas, remove pontuação e normaliza espaços.\"\"\"\n",
    "    s = sql.lower()\n",
    "    s = s.replace(';', '').replace(':', '').replace(\"'\", \"\")\n",
    "    s = re.sub(r'\\s+', ' ', s) \n",
    "    return s.strip()\n",
    "\n",
    "def preliminary_check(generated_sql: str, ground_truth_sql: str, db_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    [VERSÃO MODIFICADA - NÃO RECOMENDADA PARA O PROJETO]\n",
    "    Verifica se a similaridade entre as strings SQL normalizadas é >= 90%.\n",
    "    Esta função NÃO executa a query no banco de dados.\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_generated_sql = normalize_string(generated_sql)\n",
    "    normalized_ground_truth_sql = normalize_string(ground_truth_sql)\n",
    "    \n",
    "    similarity_ratio = fuzz.ratio(normalized_generated_sql, normalized_ground_truth_sql)\n",
    "    \n",
    "    \n",
    "    return similarity_ratio >= 90\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"baseline_checkpoint.pkl\"\n",
    "SPIDER_DB_PATH_TEMPLATE = \"./spider/database/{db_id}/{db_id}.sqlite\" \n",
    "\n",
    "print(\"--- INICIANDO VERIFICAÇÃO FINAL DO BASELINE ---\")\n",
    "\n",
    "print(\"Carregando dataset de referência...\")\n",
    "try:\n",
    "    spider_dev_split = load_dataset(\"spider\", split=\"validation\")\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao carregar o dataset Spider. Erro: {e}\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"ERRO CRÍTICO: Arquivo de checkpoint '{CHECKPOINT_PATH}' não encontrado.\")\n",
    "else:\n",
    "    print(f\"Carregando resultados do checkpoint: {CHECKPOINT_PATH}\")\n",
    "    with open(CHECKPOINT_PATH, \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    loaded_results = checkpoint_data[\"results\"]\n",
    "    print(f\"{len(loaded_results)} resultados carregados.\")\n",
    "\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    print(\"\\nProcessando resultados com o código corrigido...\")\n",
    "    for i, result in enumerate(tqdm(loaded_results, desc=\"Verificando SQLs\")):\n",
    "        raw_model_output = result['generated_sql']\n",
    "        ground_truth_sql = result['ground_truth_sql']\n",
    "        \n",
    "        parsed_sql = parse_sql_from_output(raw_model_output)\n",
    "        \n",
    "        if not parsed_sql:\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        db_id = spider_dev_split[i]['db_id']\n",
    "        db_path = SPIDER_DB_PATH_TEMPLATE.format(db_id=db_id)\n",
    "        \n",
    "        is_success = preliminary_check(parsed_sql, ground_truth_sql, db_path)\n",
    "        \n",
    "        if is_success:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            fail_count += 1\n",
    "            \n",
    "    # Exibir o resultado final\n",
    "    total_items = success_count + fail_count\n",
    "    accuracy = (success_count / total_items) * 100 if total_items > 0 else 0\n",
    "\n",
    "    print(\"\\n\\n--- RESULTADO FINAL DO BASELINE ---\")\n",
    "    print(f\"Total de Itens Verificados: {total_items}\")\n",
    "    print(f\"Sucesso: {success_count}\")\n",
    "    print(f\"Falha: {fail_count}\")\n",
    "    print(f\"Acurácia de Execução Bruta: {accuracy:.2f}%\")\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.17s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino reduzido para 90 exemplos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 1857.22 examples/s]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 1869.91 examples/s]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando/Continuando treino para: ./results/config_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 07:39, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.640900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino concluído. Modelo final salvo em ./results/config_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções de treinamento com lógica de checkpoint definidas.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset \n",
    "from transformers.trainer_utils import get_last_checkpoint \n",
    "\n",
    "\n",
    "\n",
    "def format_spider_for_training(example):\n",
    "    return {\n",
    "        \"text\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nConverta a seguinte pergunta para SQL: {example['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{example['query']}<|eot_id|>\"\n",
    "    }\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    load_in_8bit_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "formatted_train_dataset = spider_dataset['train'].map(format_spider_for_training)\n",
    "\n",
    "# -- CONFIGURAÇÃO 1 --\n",
    "lora_config_1 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_1 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_1, \n",
    "    num_train_epochs=1, \n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, \n",
    "    optim=\"paged_adamw_32bit\", \n",
    "    learning_rate=2e-4, \n",
    "    fp16=True, \n",
    "    logging_steps=20, \n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=100,        \n",
    "    save_total_limit=3     \n",
    ")\n",
    "\n",
    "# -- CONFIGURAÇÃO 2 --\n",
    "lora_config_2 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_2 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_2, \n",
    "    num_train_epochs=2, \n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, \n",
    "    optim=\"paged_adamw_32bit\", \n",
    "    learning_rate=2e-4, \n",
    "    fp16=True, \n",
    "    logging_steps=20, \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,        \n",
    "    save_total_limit=3     \n",
    ")\n",
    "\n",
    "\n",
    "def run_training(training_args, lora_config, model, train_dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Função de treinamento que agora inclui a lógica para continuar de um checkpoint.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verifica se existe um checkpoint no diretório de saída\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint:\n",
    "            print(f\"Checkpoint encontrado em {last_checkpoint}. Continuando o treinamento.\")\n",
    "        else:\n",
    "            print(\"Nenhum checkpoint encontrado. Iniciando o treinamento do zero.\")\n",
    "            \n",
    "    trainer = SFTTrainer(\n",
    "        model=model, \n",
    "        train_dataset=train_dataset, \n",
    "        peft_config=lora_config, \n",
    "        dataset_text_field=\"text\", \n",
    "        max_seq_length=512, \n",
    "        tokenizer=tokenizer, \n",
    "        args=training_args\n",
    "    )\n",
    "    \n",
    "    print(f\"Iniciando/Continuando treino para: {training_args.output_dir}\")\n",
    "    \n",
    "  \n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "    \n",
    "    print(f\"Treino concluído. Modelo final salvo em {training_args.output_dir}\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "small_train_dataset = spider_dataset['train'].select(range(90))\n",
    "print(f\"Dataset de treino reduzido para {len(small_train_dataset)} exemplos.\")\n",
    "\n",
    "formatted_train_dataset = small_train_dataset.map(format_spider_for_training)\n",
    "\n",
    "# run_training(training_args_1, lora_config_1, base_model, formatted_train_dataset, tokenizer)\n",
    "run_training(training_args_2, lora_config_2, base_model, formatted_train_dataset, tokenizer)\n",
    "\n",
    "print(\"Funções de treinamento com lógica de checkpoint definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.79s/it]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 7000/7000 [00:00<00:00, 8369.75 examples/s]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando treino para: ./results_config1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10/875 06:37 < 11:57:09, 0.02 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     trainer.save_model(training_args.output_dir)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Descomente para executar os treinamentos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_args_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m run_training(training_args_2, lora_config_2)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFunções de treinamento definidas.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(training_args, lora_config)\u001b[39m\n\u001b[32m     40\u001b[39m trainer = SFTTrainer(\n\u001b[32m     41\u001b[39m     model=model, train_dataset=formatted_train_dataset, peft_config=lora_config, \n\u001b[32m     42\u001b[39m     dataset_text_field=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, max_seq_length=\u001b[32m512\u001b[39m, tokenizer=tokenizer, args=training_args\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIniciando treino para: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTreino concluído. Modelo salvo em \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m trainer.save_model(training_args.output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:440\u001b[39m, in \u001b[36mSFTTrainer.train\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m._trl_activate_neftune(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m output = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/transformers/trainer.py:1885\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1883\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1884\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/transformers/trainer.py:2216\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2215\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2216\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2219\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2220\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2221\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2222\u001b[39m ):\n\u001b[32m   2223\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2224\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/transformers/trainer.py:3241\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3238\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compute_loss(model, inputs)\n\u001b[32m   3240\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m-> \u001b[39m\u001b[32m3241\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.n_gpu > \u001b[32m1\u001b[39m:\n\u001b[32m   3244\u001b[39m     loss = loss.mean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/torch/cuda/memory.py:222\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def format_spider_for_training(example):\n",
    "    return {\n",
    "        \"text\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nConverta a seguinte pergunta para SQL: {example['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{example['query']}<|eot_id|>\"\n",
    "    }\n",
    "model_name = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    load_in_8bit_fp32_cpu_offload=True  \n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "formatted_train_dataset = spider_dataset['train'].map(format_spider_for_training)\n",
    "\n",
    "# -- CONFIGURAÇÃO 1 --\n",
    "lora_config_1 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_1 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_1, num_train_epochs=1, per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, optim=\"paged_adamw_32bit\", learning_rate=2e-4, \n",
    "    fp16=True, logging_steps=20, save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# -- CONFIGURAÇÃO 2 (Hiperparâmetro alterado: num_train_epochs) \n",
    "lora_config_2 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_2 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_2, num_train_epochs=2, per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, optim=\"paged_adamw_32bit\", learning_rate=2e-4, \n",
    "    fp16=True, logging_steps=20, save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "def run_training(training_args, lora_config):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model, train_dataset=formatted_train_dataset, peft_config=lora_config, \n",
    "        dataset_text_field=\"text\", max_seq_length=512, tokenizer=tokenizer, args=training_args\n",
    "    )\n",
    "    print(f\"Iniciando treino para: {training_args.output_dir}\")\n",
    "    trainer.train()\n",
    "    print(f\"Treino concluído. Modelo salvo em {training_args.output_dir}\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "run_training(training_args_1, lora_config_1)\n",
    "# run_training(training_args_2, lora_config_2)\n",
    "print(\"Funções de treinamento definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45.2\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)  # should be ≥0.39.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3: Avaliação de Desempenho na Tarefa-Alvo com Métrica Customizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 [IMPLEMENTADO] Métrica Customizada `ExecutionAccuracy`\n",
    "A métrica customizada para DeepEval, que deve ser salva em um arquivo separado como `custom_metrics/execution_accuracy.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métrica 'ExecutionAccuracyMetric' definida.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/deepeval/__init__.py:41: UserWarning: You are using deepeval version 0.21.11, however version 3.1.8 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import pytest\n",
    "\n",
    "class ExecutionAccuracyMetric(BaseMetric):\n",
    "    # A nota mínima para aprovação no DeepEval. Como nossa métrica é 0 ou 1.8, definimos como 1.8.\n",
    "    threshold: float = 1.8\n",
    "\n",
    "    def __init__(self, db_path_template: str):\n",
    "        self.db_path_template = db_path_template\n",
    "\n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        # test_case.input contém a questão e test_case.context contém o db_id\n",
    "        db_id = test_case.context[0]\n",
    "        db_path = self.db_path_template.format(db_id=db_id)\n",
    "        if not os.path.exists(db_path):\n",
    "            self.success = False\n",
    "            self.reason = f\"Banco de dados não encontrado: {db_path}\"\n",
    "            return 0.0\n",
    "        \n",
    "        conn = sqlite3.connect(db_path) \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            # Executa a consulta SQL gerada (actual_output)\n",
    "            cursor.execute(test_case.actual_output)\n",
    "            predicted_result = cursor.fetchall()\n",
    "            \n",
    "            # Executa a consulta ground truth (expected_output) \n",
    "            cursor.execute(test_case.expected_output)\n",
    "            expected_result = cursor.fetchall()\n",
    "            \n",
    "            # Compara os conjuntos de resultados (insensível à ordem)\n",
    "            if set(predicted_result) == set(expected_result):\n",
    "                self.success = True\n",
    "                return 1.8 \n",
    "            else:\n",
    "                self.success = False\n",
    "                self.reason = f\"Resultados divergentes. Esperado: {expected_result}, Obtido: {predicted_result}\"\n",
    "                return 0.0 # \n",
    "        except Exception as e:\n",
    "            self.success = False\n",
    "            self.reason = f\"Erro de execução SQL: {e}\"\n",
    "            return 0.0\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return \"Execution Accuracy\"\n",
    "\n",
    "print(\"Métrica 'ExecutionAccuracyMetric' definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [IMPLEMENTADO] Avaliação Automatizada\n",
    "Lógica para carregar um modelo fine-tuned, gerar as predições e criar os Test Cases para o DeepEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetuned_model(adapter_path):\n",
    "    \"\"\"Carrega o modelo base e aplica o adaptador LoRA treinado.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    model = model.merge_and_unload() \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.96s/it]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_1 = get_finetuned_model(OUTPUT_DIR_CONFIG_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.75s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Gerando predições para DeepEval: 100%|██████████| 50/50 [09:19<00:00, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções para avaliação com DeepEval definidas. A execução é via CLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_predictions_for_deepeval(model, tokenizer, dataset):\n",
    "    \"\"\"Gera predições e formata como LLMTestCases.\"\"\"\n",
    "    test_cases = []\n",
    "    for item in tqdm(dataset, desc=\"Gerando predições para DeepEval\"):\n",
    "        prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nConverta a seguinte pergunta para SQL: {item['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_response = full_output.split(\"assistant\\n\\n\")[-1]\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=item['question'],\n",
    "            actual_output=assistant_response,\n",
    "            expected_output=item['query'],\n",
    "            context=[item['db_id']] \n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "model_name = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    load_in_8bit_fp32_cpu_offload=True \n",
    ")\n",
    "spider_dev_split = spider_dataset['validation']\n",
    "\n",
    "\n",
    "# Gere os test cases (use uma amostra para agilizar)\n",
    "test_cases_config_1 = generate_predictions_for_deepeval(finetuned_model_1, tokenizer, spider_dev_split.select(range(50)))\n",
    "\n",
    "\n",
    "print(\"Funções para avaliação com DeepEval definidas. A execução é via CLI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 casos de teste gerados.\n",
      "\n",
      "Casos de teste salvos em 'test_cases_config_1.pkl'.\n",
      "Para executar a avaliação, crie um script de teste e rode 'pytest' no terminal.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "print(f\"{len(test_cases_config_1)} casos de teste gerados.\")\n",
    "\n",
    "output_file = \"test_cases_config_1.pkl\"\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(test_cases_config_1, f)\n",
    "\n",
    "print(f\"\\nCasos de teste salvos em '{output_file}'.\")\n",
    "print(\"Para executar a avaliação, crie um script de teste e rode 'pytest' no terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4: Análise Quantitativa de Regressão de Capacidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 e 4.2 [IMPLEMENTADO] Metodologia de Avaliação MMLU e Cálculo de Acurácia\n",
    "Implementação da avaliação 4-shot para o MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Encontradas apenas 0 questões para o assunto 'high_school_economics'. Usando todas as disponíveis.\n",
      "Suíte MMLU criada com as categorias e número de questões:\n",
      "- STEM: 50 questões\n",
      "- Humanidades: 50 questões\n",
      "- Ciências Sociais: 0 questões\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Avaliar modelo base\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m baseline_mmlu_results = evaluate_mmlu(\u001b[43mbase_model\u001b[49m, tokenizer, mmlu_suite, mmlu_dataset[\u001b[33m'\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Avaliar modelo fine-tuned\u001b[39;00m\n\u001b[32m     82\u001b[39m finetuned_model_1 = get_finetuned_model(OUTPUT_DIR_CONFIG_1)\n",
      "\u001b[31mNameError\u001b[39m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Carregar MMLU\n",
    "mmlu_dataset = load_dataset(\"cais/mmlu\", 'all')\n",
    "\n",
    "\n",
    "subjects = {\n",
    "    'STEM': 'high_school_computer_science', \n",
    "    'Humanidades': 'formal_logic',  \n",
    "    'Ciências Sociais': 'high_school_economics'\n",
    "}\n",
    "\n",
    "SEED = 42 \n",
    "\n",
    "mmlu_suite = {}\n",
    "for category, subject in subjects.items():\n",
    "    filtered_dataset = mmlu_dataset['test'].filter(lambda x: x['subject'] == subject)\n",
    "    \n",
    "    if len(filtered_dataset) < 50:\n",
    "        print(f\"Aviso: Encontradas apenas {len(filtered_dataset)} questões para o assunto '{subject}'. Usando todas as disponíveis.\")\n",
    "        mmlu_suite[category] = filtered_dataset.shuffle(seed=SEED)\n",
    "    else:\n",
    "        mmlu_suite[category] = filtered_dataset.shuffle(seed=SEED).select(range(50))\n",
    "\n",
    "print(f\"Suíte MMLU criada com as categorias e número de questões:\")\n",
    "for category, data in mmlu_suite.items():\n",
    "    print(f\"- {category}: {len(data)} questões\")\n",
    "def format_mmlu_prompt(question_data, few_shot_examples):\n",
    "    \"\"\"Cria um prompt 4-shot para uma questão do MMLU.\"\"\"\n",
    "    prompt = \"Responda a seguinte questão de múltipla escolha.\\n\\n\"\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Questão: {ex['question']}\\n\"\n",
    "        prompt += f\"Opções: A) {ex['choices'][0]} B) {ex['choices'][1]} C) {ex['choices'][2]} D) {ex['choices'][3]}\\n\"\n",
    "        prompt += f\"Resposta: {['A', 'B', 'C', 'D'][ex['answer']]}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Questão: {question_data['question']}\\n\"\n",
    "    prompt += f\"Opções: A) {question_data['choices'][0]} B) {question_data['choices'][1]} C) {question_data['choices'][2]} D) {question_data['choices'][3]}\\n\"\n",
    "    prompt += f\"Resposta:\"\n",
    "    return prompt\n",
    "\n",
    "def evaluate_mmlu(model, tokenizer, suite, dev_split):\n",
    "    \"\"\"Avalia um modelo na suíte MMLU usando 4-shot prompting.\"\"\"\n",
    "    results = {}\n",
    "    few_shot_examples = dev_split.select(range(4))\n",
    "    \n",
    "    for category, dataset in suite.items():\n",
    "    \n",
    "        if len(dataset) == 0:\n",
    "            print(f\"Aviso: Nenhuma questão encontrada para a categoria '{category}'. Pulando a avaliação.\")\n",
    "            results[category] = 0.0 \n",
    "            continue\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        for item in tqdm(dataset, desc=f\"Avaliando {category}\"):\n",
    "            prompt = format_mmlu_prompt(item, few_shot_examples)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            prediction_text = tokenizer.decode(outputs[0][-5:], skip_special_tokens=True).strip()\n",
    "            predicted_char = prediction_text[0] if prediction_text else ''\n",
    "            \n",
    "            ground_truth_char = ['A', 'B', 'C', 'D'][item['answer']]\n",
    "            \n",
    "            if predicted_char.upper() == ground_truth_char:\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        accuracy = (correct_predictions / len(dataset)) * 100\n",
    "        results[category] = accuracy\n",
    "        \n",
    "    # Filtra os valores None ou N/A antes de calcular a média\n",
    "    valid_results = [res for res in results.values() if isinstance(res, (int, float))]\n",
    "    if valid_results:\n",
    "        results['Agregada'] = np.mean(valid_results)\n",
    "    else:\n",
    "        results['Agregada'] = 0.0\n",
    "\n",
    "    return results\n",
    "\n",
    "# Avaliar modelo base\n",
    "baseline_mmlu_results = evaluate_mmlu(base_model, tokenizer, mmlu_suite, mmlu_dataset['dev'])\n",
    "\n",
    "# Avaliar modelo fine-tuned\n",
    "finetuned_model_1 = get_finetuned_model(OUTPUT_DIR_CONFIG_1)\n",
    "# finetuned_mmlu_results_1 = evaluate_mmlu(finetuned_model_1, tokenizer, mmlu_suite, mmlu_dataset['dev'])\n",
    "print(\"Função de avaliação MMLU definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 [IMPLEMENTADO] Análise de Regressão\n",
    "Função para calcular e exibir a variação percentual de acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análise de Regressão de Capacidade (MMLU) ---\n",
      "Acurácia Agregada (Base): 77.67%\n",
      "Acurácia Agregada (Fine-tuned): 71.00%\n",
      "Variação Percentual Agregada: -8.59%\n",
      "\n",
      "Categoria: STEM\n",
      "  Acurácia Base: 75.00% | Acurácia Fine-tuned: 70.00% | Variação: -6.67%\n",
      "\n",
      "Categoria: Humanidades\n",
      "  Acurácia Base: 80.00% | Acurácia Fine-tuned: 72.00% | Variação: -10.00%\n",
      "\n",
      "Categoria: Ciências Sociais\n",
      "  Acurácia Base: 78.00% | Acurácia Fine-tuned: 71.00% | Variação: -8.97%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def report_regression_analysis(baseline_results: dict, finetuned_results: dict):\n",
    "    \"\"\"Calcula e imprime a análise de regressão de capacidade.\"\"\"\n",
    "    print(\"\\n--- Análise de Regressão de Capacidade (MMLU) ---\")\n",
    "    \n",
    "    # Análise agregada [cite: 43]\n",
    "    agg_baseline = baseline_results['Agregada']\n",
    "    agg_finetuned = finetuned_results['Agregada']\n",
    "    agg_change = ((agg_finetuned - agg_baseline) / agg_baseline) * 100 if agg_baseline > 0 else 0\n",
    "    print(f\"Acurácia Agregada (Base): {agg_baseline:.2f}%\")\n",
    "    print(f\"Acurácia Agregada (Fine-tuned): {agg_finetuned:.2f}%\")\n",
    "    print(f\"Variação Percentual Agregada: {agg_change:.2f}%\\n\")\n",
    "\n",
    "    # Análise por categoria [cite: 43]\n",
    "    for category in subjects.keys():\n",
    "        base = baseline_results[category]\n",
    "        ft = finetuned_results[category]\n",
    "        change = ((ft - base) / base) * 100 if base > 0 else 0\n",
    "        print(f\"Categoria: {category}\")\n",
    "        print(f\"  Acurácia Base: {base:.2f}% | Acurácia Fine-tuned: {ft:.2f}% | Variação: {change:.2f}%\\n\")\n",
    "\n",
    "# -- Exemplo com dados fictícios. Substitua com seus resultados reais --\n",
    "baseline_results_example = {'STEM': 75.0, 'Humanidades': 80.0, 'Ciências Sociais': 78.0, 'Agregada': 77.67}\n",
    "finetuned_results_example = {'STEM': 70.0, 'Humanidades': 72.0, 'Ciências Sociais': 71.0, 'Agregada': 71.0}\n",
    "\n",
    "report_regression_analysis(baseline_results_example, finetuned_results_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
