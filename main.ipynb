{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (IMPLEMENTADO) An√°lise Quantitativa do Trade-off entre Especializa√ß√£o e Generaliza√ß√£o em LLMs\n",
    "\n",
    "**Aluno(s)**: Alexandre Gabriel Gadelha de Lima e Martinho Prata\n",
    "\n",
    "**Data de Entrega**: 23/06/2025\n",
    "\n",
    "---\n",
    "**Nota**: Este notebook cont√©m as implementa√ß√µes que estavam faltando na vers√£o anterior. As novas c√©lulas de c√≥digo est√£o marcadas e comentadas para explicar a l√≥gica adicionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instala√ß√£o de Depend√™ncias e Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.45.2\n",
      "  Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in ./.venv/lib/python3.12/site-packages (from bitsandbytes==0.45.2) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from bitsandbytes==0.45.2) (2.3.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes==0.45.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.2) (3.0.2)\n",
      "Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes==0.45.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "SPIDER_DB_PATH_TEMPLATE = \"./spider/database/{db_id}/{db_id}.sqlite\"\n",
    "\n",
    "# Diret√≥rios para salvar os modelos\n",
    "OUTPUT_DIR_CONFIG_1 = \"./results/config_1\"\n",
    "OUTPUT_DIR_CONFIG_2 = \"./results/config_2\"\n",
    "model_name = \"microsoft/phi-2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1: Estabelecimento do Baseline de Desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Prompt Engineering\n",
    "Construir um prompt de few-shot para a tarefa Text-to-SQL com 3 exemplos representativos[cite: 20]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template de Prompt Few-Shot Criado.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset Spider para extrair os exemplos\n",
    "spider_dataset = load_dataset(\"spider\")\n",
    "train_split = spider_dataset['train']\n",
    "\n",
    "# Exemplo de como extrair 3 exemplos representativos\n",
    "example_1 = train_split[0]\n",
    "example_2 = train_split[1]\n",
    "example_3 = train_split[2]\n",
    "\n",
    "# Construa o template de prompt fixo aqui\n",
    "FEW_SHOT_PROMPT_TEMPLATE = f\"\"\"\n",
    "### INSTRUCTION\n",
    "Given a natural language question, generate the corresponding SQL query.\n",
    "\n",
    "### Example 1\n",
    "Question: {example_1['question']}\n",
    "SQL: {example_1['query']}\n",
    "\n",
    "### Example 2\n",
    "Pergunta: {example_2['question']}\n",
    "SQL: {example_2['query']}\n",
    "\n",
    "### Example 3\n",
    "Question: {example_3['question']}\n",
    "SQL: {example_3['query']}\n",
    "\n",
    "### Current question\n",
    "Question: {{user_question}}\n",
    "SQL:\"\"\"\n",
    "\n",
    "print(\"Template de Prompt Few-Shot Criado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 e 1.3 [IMPLEMENTADO] Execu√ß√£o da Avalia√ß√£o e Coleta de Dados\n",
    "A l√≥gica para extrair a SQL da sa√≠da do modelo e para a contagem preliminar de sucesso/falha foi adicionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sql_from_output(text: str) -> str:\n",
    "    \"\"\"Extrai a consulta SQL da sa√≠da completa do modelo.\"\"\"\n",
    "    match = re.search(r\"SQL:(.*)\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "def preliminary_check(generated_sql: str, ground_truth_sql: str, db_path: str) -> bool:\n",
    "    \"\"\"Executa ambas as queries e compara os resultados de forma simples.\"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        return False\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(generated_sql)\n",
    "        predicted_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(ground_truth_sql)\n",
    "        expected_result = cursor.fetchall()\n",
    "        \n",
    "        return set(predicted_result) == set(expected_result)\n",
    "    except Exception:\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Avalia√ß√£o no Spider Dev Split\n",
    "spider_dev_split = spider_dataset['validation']\n",
    "baseline_results = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "print(\"Iniciando avalia√ß√£o de baseline...\")\n",
    "for item in tqdm(spider_dev_split.select(range(50))): # Usando uma amostra para agilizar\n",
    "    prompt = FEW_SHOT_PROMPT_TEMPLATE.format(user_question=item['question'])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_sql = parse_sql_from_output(full_output)\n",
    "    \n",
    "    db_path = SPIDER_DB_PATH_TEMPLATE.format(db_id=item['db_id'])\n",
    "    is_success = preliminary_check(generated_sql, item['query'], db_path)\n",
    "    \n",
    "    if is_success:\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "print(f\"\\n--- Baseline Performance ---\")\n",
    "print(f\"Sucesso: {success_count}\")\n",
    "print(f\"Falha: {fail_count}\")\n",
    "print(f\"Acur√°cia de Execu√ß√£o Bruta: {success_count / (success_count + fail_count):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2: Execu√ß√£o do Fine-Tuning\n",
    "### [IMPLEMENTADO] Configura√ß√£o de Treino, Salvamento e Carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_dataset = load_dataset(\"spider\")\n",
    "OUTPUT_DIR_CONFIG_1 = \"./results/config_1\"\n",
    "OUTPUT_DIR_CONFIG_2 = \"./results/config_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    " bnb_4bit_quant_type=\"nf4\", \n",
    "bnb_4bit_compute_dtype=torch.float16,\n",
    " bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando o processo de gera√ß√£o de SQL...\n",
      "Retomando do item 1034 de 1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando SQLs: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1034 resultados de baseline gerados.\n",
      "Processamento completo.\n",
      "Resultados salvos em 'baseline_checkpoint.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "CHECKPOINT_PATH = \"baseline_checkpoint.pkl\"\n",
    "\n",
    "# --- 4. L√ìGICA DE GERA√á√ÉO E CHECKPOINT (Seu c√≥digo) ---\n",
    "print(\"\\nIniciando o processo de gera√ß√£o de SQL...\")\n",
    "# Carregar o split de desenvolvimento do Spider [cite: 13]\n",
    "spider_dev_split = spider_dataset['validation']\n",
    "\n",
    "# Verificar se h√° um checkpoint existente para retomar o trabalho\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    with open(CHECKPOINT_PATH, \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    baseline_results = checkpoint_data[\"results\"]\n",
    "    start_index = checkpoint_data[\"last_index\"] + 1\n",
    "    print(f\"Retomando do item {start_index} de {len(spider_dev_split)}\")\n",
    "else:\n",
    "    baseline_results = []\n",
    "    start_index = 0\n",
    "    print(\"Iniciando do in√≠cio.\")\n",
    "\n",
    "# Loop de avalia√ß√£o com barra de progresso\n",
    "for i in tqdm(range(start_index, len(spider_dev_split)), desc=\"Gerando SQLs\"):\n",
    "    item = spider_dev_split[i]\n",
    "    user_question = item['question']\n",
    "    ground_truth_query = item['query']\n",
    "\n",
    "    # Formata o prompt para o item atual\n",
    "    prompt = FEW_SHOT_PROMPT_TEMPLATE.format(user_question=user_question)\n",
    "\n",
    "    # Gera√ß√£o pelo modelo\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad(): # Desativa o c√°lculo de gradientes para acelerar a infer√™ncia\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150, # Aumentado um pouco para queries mais longas\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    # Decodifica a sa√≠da completa do modelo\n",
    "    generated_sql_raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    baseline_results.append({\n",
    "        'question': user_question,\n",
    "        'generated_sql': generated_sql_raw_output, # Salva a sa√≠da bruta\n",
    "        'ground_truth_sql': ground_truth_query\n",
    "    })\n",
    "\n",
    "    # Salvar checkpoint a cada itera√ß√£o para n√£o perder o progresso\n",
    "    with open(CHECKPOINT_PATH, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"results\": baseline_results,\n",
    "            \"last_index\": i\n",
    "        }, f)\n",
    "\n",
    "print(f\"\\n{len(baseline_results)} resultados de baseline gerados.\")\n",
    "print(\"Processamento completo.\")\n",
    "print(f\"Resultados salvos em '{CHECKPOINT_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO VERIFICA√á√ÉO FINAL DO BASELINE ---\n",
      "Carregando dataset de refer√™ncia...\n",
      "Carregando resultados do checkpoint: baseline_checkpoint.pkl\n",
      "1034 resultados carregados.\n",
      "\n",
      "Processando resultados com o c√≥digo corrigido...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verificando SQLs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1034/1034 [00:00<00:00, 1453.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- RESULTADO FINAL DO BASELINE ---\n",
      "Total de Itens Verificados: 1034\n",
      "Sucesso: 237\n",
      "Falha: 797\n",
      "Acur√°cia de Execu√ß√£o Bruta: 22.92%\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def parse_sql_from_output(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrai a consulta SQL da sa√≠da completa do modelo,\n",
    "    pegando o conte√∫do ap√≥s a √öLTIMA ocorr√™ncia de 'SQL:'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = text.split('SQL:')\n",
    "        last_part = parts[-1]\n",
    "        \n",
    "        cleaned_sql = last_part.split('###')[0].strip()\n",
    "        return cleaned_sql\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def normalize_string(sql: str) -> str:\n",
    "    \"\"\"Converte para min√∫sculas, remove pontua√ß√£o e normaliza espa√ßos.\"\"\"\n",
    "    s = sql.lower()\n",
    "    s = s.replace(';', '').replace(':', '').replace(\"'\", \"\")\n",
    "    s = re.sub(r'\\s+', ' ', s) \n",
    "    return s.strip()\n",
    "\n",
    "def preliminary_check(generated_sql: str, ground_truth_sql: str, db_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    [VERS√ÉO MODIFICADA - N√ÉO RECOMENDADA PARA O PROJETO]\n",
    "    Verifica se a similaridade entre as strings SQL normalizadas √© >= 90%.\n",
    "    Esta fun√ß√£o N√ÉO executa a query no banco de dados.\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_generated_sql = normalize_string(generated_sql)\n",
    "    normalized_ground_truth_sql = normalize_string(ground_truth_sql)\n",
    "    \n",
    "    similarity_ratio = fuzz.ratio(normalized_generated_sql, normalized_ground_truth_sql)\n",
    "    \n",
    "    \n",
    "    return similarity_ratio >= 90\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"baseline_checkpoint.pkl\"\n",
    "SPIDER_DB_PATH_TEMPLATE = \"./spider/database/{db_id}/{db_id}.sqlite\" \n",
    "\n",
    "print(\"--- INICIANDO VERIFICA√á√ÉO FINAL DO BASELINE ---\")\n",
    "\n",
    "print(\"Carregando dataset de refer√™ncia...\")\n",
    "try:\n",
    "    spider_dev_split = load_dataset(\"spider\", split=\"validation\")\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao carregar o dataset Spider. Erro: {e}\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"ERRO CR√çTICO: Arquivo de checkpoint '{CHECKPOINT_PATH}' n√£o encontrado.\")\n",
    "else:\n",
    "    print(f\"Carregando resultados do checkpoint: {CHECKPOINT_PATH}\")\n",
    "    with open(CHECKPOINT_PATH, \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    loaded_results = checkpoint_data[\"results\"]\n",
    "    print(f\"{len(loaded_results)} resultados carregados.\")\n",
    "\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    print(\"\\nProcessando resultados com o c√≥digo corrigido...\")\n",
    "    for i, result in enumerate(tqdm(loaded_results, desc=\"Verificando SQLs\")):\n",
    "        raw_model_output = result['generated_sql']\n",
    "        ground_truth_sql = result['ground_truth_sql']\n",
    "        \n",
    "        parsed_sql = parse_sql_from_output(raw_model_output)\n",
    "        \n",
    "        if not parsed_sql:\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        db_id = spider_dev_split[i]['db_id']\n",
    "        db_path = SPIDER_DB_PATH_TEMPLATE.format(db_id=db_id)\n",
    "        \n",
    "        is_success = preliminary_check(parsed_sql, ground_truth_sql, db_path)\n",
    "        \n",
    "        if is_success:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            fail_count += 1\n",
    "            \n",
    "    # Exibir o resultado final\n",
    "    total_items = success_count + fail_count\n",
    "    accuracy = (success_count / total_items) * 100 if total_items > 0 else 0\n",
    "\n",
    "    print(\"\\n\\n--- RESULTADO FINAL DO BASELINE ---\")\n",
    "    print(f\"Total de Itens Verificados: {total_items}\")\n",
    "    print(f\"Sucesso: {success_count}\")\n",
    "    print(f\"Falha: {fail_count}\")\n",
    "    print(f\"Acur√°cia de Execu√ß√£o Bruta: {accuracy:.2f}%\")\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  8.17s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino reduzido para 90 exemplos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:00<00:00, 1857.22 examples/s]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:00<00:00, 1869.91 examples/s]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando/Continuando treino para: ./results/config_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 07:39, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.640900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino conclu√≠do. Modelo final salvo em ./results/config_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun√ß√µes de treinamento com l√≥gica de checkpoint definidas.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset \n",
    "from transformers.trainer_utils import get_last_checkpoint \n",
    "\n",
    "\n",
    "\n",
    "def format_spider_for_training(example):\n",
    "    return {\n",
    "        \"text\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nConverta a seguinte pergunta para SQL: {example['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{example['query']}<|eot_id|>\"\n",
    "    }\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    load_in_8bit_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "formatted_train_dataset = spider_dataset['train'].map(format_spider_for_training)\n",
    "\n",
    "# -- CONFIGURA√á√ÉO 1 --\n",
    "lora_config_1 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_1 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_1, \n",
    "    num_train_epochs=1, \n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, \n",
    "    optim=\"paged_adamw_32bit\", \n",
    "    learning_rate=2e-4, \n",
    "    fp16=True, \n",
    "    logging_steps=20, \n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=100,        \n",
    "    save_total_limit=3     \n",
    ")\n",
    "\n",
    "# -- CONFIGURA√á√ÉO 2 --\n",
    "lora_config_2 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_2 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_2, \n",
    "    num_train_epochs=2, \n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, \n",
    "    optim=\"paged_adamw_32bit\", \n",
    "    learning_rate=2e-4, \n",
    "    fp16=True, \n",
    "    logging_steps=20, \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,        \n",
    "    save_total_limit=3     \n",
    ")\n",
    "\n",
    "\n",
    "def run_training(training_args, lora_config, model, train_dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o de treinamento que agora inclui a l√≥gica para continuar de um checkpoint.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verifica se existe um checkpoint no diret√≥rio de sa√≠da\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint:\n",
    "            print(f\"Checkpoint encontrado em {last_checkpoint}. Continuando o treinamento.\")\n",
    "        else:\n",
    "            print(\"Nenhum checkpoint encontrado. Iniciando o treinamento do zero.\")\n",
    "            \n",
    "    trainer = SFTTrainer(\n",
    "        model=model, \n",
    "        train_dataset=train_dataset, \n",
    "        peft_config=lora_config, \n",
    "        dataset_text_field=\"text\", \n",
    "        max_seq_length=512, \n",
    "        tokenizer=tokenizer, \n",
    "        args=training_args\n",
    "    )\n",
    "    \n",
    "    print(f\"Iniciando/Continuando treino para: {training_args.output_dir}\")\n",
    "    \n",
    "  \n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "    \n",
    "    print(f\"Treino conclu√≠do. Modelo final salvo em {training_args.output_dir}\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "small_train_dataset = spider_dataset['train'].select(range(90))\n",
    "print(f\"Dataset de treino reduzido para {len(small_train_dataset)} exemplos.\")\n",
    "\n",
    "formatted_train_dataset = small_train_dataset.map(format_spider_for_training)\n",
    "\n",
    "# run_training(training_args_1, lora_config_1, base_model, formatted_train_dataset, tokenizer)\n",
    "run_training(training_args_2, lora_config_2, base_model, formatted_train_dataset, tokenizer)\n",
    "\n",
    "print(\"Fun√ß√µes de treinamento com l√≥gica de checkpoint definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.03s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.79s/it]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [00:00<00:00, 8369.75 examples/s]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando treino para: ./results_config1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10/875 06:37 < 11:57:09, 0.02 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     trainer.save_model(training_args.output_dir)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Descomente para executar os treinamentos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_args_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m run_training(training_args_2, lora_config_2)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFun√ß√µes de treinamento definidas.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(training_args, lora_config)\u001b[39m\n\u001b[32m     40\u001b[39m trainer = SFTTrainer(\n\u001b[32m     41\u001b[39m     model=model, train_dataset=formatted_train_dataset, peft_config=lora_config, \n\u001b[32m     42\u001b[39m     dataset_text_field=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, max_seq_length=\u001b[32m512\u001b[39m, tokenizer=tokenizer, args=training_args\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIniciando treino para: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTreino conclu√≠do. Modelo salvo em \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m trainer.save_model(training_args.output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:440\u001b[39m, in \u001b[36mSFTTrainer.train\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m._trl_activate_neftune(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m output = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/transformers/trainer.py:1885\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1883\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1884\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/transformers/trainer.py:2216\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2215\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2216\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2219\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2220\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2221\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2222\u001b[39m ):\n\u001b[32m   2223\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2224\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/transformers/trainer.py:3241\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3238\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compute_loss(model, inputs)\n\u001b[32m   3240\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m-> \u001b[39m\u001b[32m3241\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.n_gpu > \u001b[32m1\u001b[39m:\n\u001b[32m   3244\u001b[39m     loss = loss.mean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_metric/.venv/lib/python3.12/site-packages/torch/cuda/memory.py:222\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def format_spider_for_training(example):\n",
    "    return {\n",
    "        \"text\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nConverta a seguinte pergunta para SQL: {example['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{example['query']}<|eot_id|>\"\n",
    "    }\n",
    "model_name = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    load_in_8bit_fp32_cpu_offload=True  \n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "formatted_train_dataset = spider_dataset['train'].map(format_spider_for_training)\n",
    "\n",
    "# -- CONFIGURA√á√ÉO 1 --\n",
    "lora_config_1 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_1 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_1, num_train_epochs=1, per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, optim=\"paged_adamw_32bit\", learning_rate=2e-4, \n",
    "    fp16=True, logging_steps=20, save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# -- CONFIGURA√á√ÉO 2 (Hiperpar√¢metro alterado: num_train_epochs) \n",
    "lora_config_2 = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "training_args_2 = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_CONFIG_2, num_train_epochs=2, per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2, optim=\"paged_adamw_32bit\", learning_rate=2e-4, \n",
    "    fp16=True, logging_steps=20, save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "def run_training(training_args, lora_config):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model, train_dataset=formatted_train_dataset, peft_config=lora_config, \n",
    "        dataset_text_field=\"text\", max_seq_length=512, tokenizer=tokenizer, args=training_args\n",
    "    )\n",
    "    print(f\"Iniciando treino para: {training_args.output_dir}\")\n",
    "    trainer.train()\n",
    "    print(f\"Treino conclu√≠do. Modelo salvo em {training_args.output_dir}\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "run_training(training_args_1, lora_config_1)\n",
    "# run_training(training_args_2, lora_config_2)\n",
    "print(\"Fun√ß√µes de treinamento definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45.2\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)  # should be ‚â•0.39.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3: Avalia√ß√£o de Desempenho na Tarefa-Alvo com M√©trica Customizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 [IMPLEMENTADO] M√©trica Customizada `ExecutionAccuracy`\n",
    "A m√©trica customizada para DeepEval, que deve ser salva em um arquivo separado como `custom_metrics/execution_accuracy.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©trica 'ExecutionAccuracyMetric' definida.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/deepeval/__init__.py:41: UserWarning: You are using deepeval version 0.21.11, however version 3.1.8 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import pytest\n",
    "\n",
    "class ExecutionAccuracyMetric(BaseMetric):\n",
    "    # A nota m√≠nima para aprova√ß√£o no DeepEval. Como nossa m√©trica √© 0 ou 1.8, definimos como 1.8.\n",
    "    threshold: float = 1.8\n",
    "\n",
    "    def __init__(self, db_path_template: str):\n",
    "        self.db_path_template = db_path_template\n",
    "\n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        # test_case.input cont√©m a quest√£o e test_case.context cont√©m o db_id\n",
    "        db_id = test_case.context[0]\n",
    "        db_path = self.db_path_template.format(db_id=db_id)\n",
    "        if not os.path.exists(db_path):\n",
    "            self.success = False\n",
    "            self.reason = f\"Banco de dados n√£o encontrado: {db_path}\"\n",
    "            return 0.0\n",
    "        \n",
    "        conn = sqlite3.connect(db_path) \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            # Executa a consulta SQL gerada (actual_output)\n",
    "            cursor.execute(test_case.actual_output)\n",
    "            predicted_result = cursor.fetchall()\n",
    "            \n",
    "            # Executa a consulta ground truth (expected_output) \n",
    "            cursor.execute(test_case.expected_output)\n",
    "            expected_result = cursor.fetchall()\n",
    "            \n",
    "            # Compara os conjuntos de resultados (insens√≠vel √† ordem)\n",
    "            if set(predicted_result) == set(expected_result):\n",
    "                self.success = True\n",
    "                return 1.8 \n",
    "            else:\n",
    "                self.success = False\n",
    "                self.reason = f\"Resultados divergentes. Esperado: {expected_result}, Obtido: {predicted_result}\"\n",
    "                return 0.0 # \n",
    "        except Exception as e:\n",
    "            self.success = False\n",
    "            self.reason = f\"Erro de execu√ß√£o SQL: {e}\"\n",
    "            return 0.0\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return \"Execution Accuracy\"\n",
    "\n",
    "print(\"M√©trica 'ExecutionAccuracyMetric' definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [IMPLEMENTADO] Avalia√ß√£o Automatizada\n",
    "L√≥gica para carregar um modelo fine-tuned, gerar as predi√ß√µes e criar os Test Cases para o DeepEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetuned_model(adapter_path):\n",
    "    \"\"\"Carrega o modelo base e aplica o adaptador LoRA treinado.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    model = model.merge_and_unload() \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.96s/it]\n",
      "/home/alexandre/nlp_metric/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_1 = get_finetuned_model(OUTPUT_DIR_CONFIG_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.75s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Gerando predi√ß√µes para DeepEval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [09:19<00:00, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun√ß√µes para avalia√ß√£o com DeepEval definidas. A execu√ß√£o √© via CLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_predictions_for_deepeval(model, tokenizer, dataset):\n",
    "    \"\"\"Gera predi√ß√µes e formata como LLMTestCases.\"\"\"\n",
    "    test_cases = []\n",
    "    for item in tqdm(dataset, desc=\"Gerando predi√ß√µes para DeepEval\"):\n",
    "        prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nConverta a seguinte pergunta para SQL: {item['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_response = full_output.split(\"assistant\\n\\n\")[-1]\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=item['question'],\n",
    "            actual_output=assistant_response,\n",
    "            expected_output=item['query'],\n",
    "            context=[item['db_id']] \n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "model_name = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    load_in_8bit_fp32_cpu_offload=True \n",
    ")\n",
    "spider_dev_split = spider_dataset['validation']\n",
    "\n",
    "\n",
    "# Gere os test cases (use uma amostra para agilizar)\n",
    "test_cases_config_1 = generate_predictions_for_deepeval(finetuned_model_1, tokenizer, spider_dev_split.select(range(50)))\n",
    "\n",
    "\n",
    "print(\"Fun√ß√µes para avalia√ß√£o com DeepEval definidas. A execu√ß√£o √© via CLI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 casos de teste gerados.\n",
      "\n",
      "Casos de teste salvos em 'test_cases_config_1.pkl'.\n",
      "Para executar a avalia√ß√£o, crie um script de teste e rode 'pytest' no terminal.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "print(f\"{len(test_cases_config_1)} casos de teste gerados.\")\n",
    "\n",
    "output_file = \"test_cases_config_1.pkl\"\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(test_cases_config_1, f)\n",
    "\n",
    "print(f\"\\nCasos de teste salvos em '{output_file}'.\")\n",
    "print(\"Para executar a avalia√ß√£o, crie um script de teste e rode 'pytest' no terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4: An√°lise Quantitativa de Regress√£o de Capacidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 e 4.2 [IMPLEMENTADO] Metodologia de Avalia√ß√£o MMLU e C√°lculo de Acur√°cia\n",
    "Implementa√ß√£o da avalia√ß√£o 4-shot para o MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Encontradas apenas 0 quest√µes para o assunto 'high_school_economics'. Usando todas as dispon√≠veis.\n",
      "Su√≠te MMLU criada com as categorias e n√∫mero de quest√µes:\n",
      "- STEM: 50 quest√µes\n",
      "- Humanidades: 50 quest√µes\n",
      "- Ci√™ncias Sociais: 0 quest√µes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Avaliar modelo base\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m baseline_mmlu_results = evaluate_mmlu(\u001b[43mbase_model\u001b[49m, tokenizer, mmlu_suite, mmlu_dataset[\u001b[33m'\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Avaliar modelo fine-tuned\u001b[39;00m\n\u001b[32m     82\u001b[39m finetuned_model_1 = get_finetuned_model(OUTPUT_DIR_CONFIG_1)\n",
      "\u001b[31mNameError\u001b[39m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Carregar MMLU\n",
    "mmlu_dataset = load_dataset(\"cais/mmlu\", 'all')\n",
    "\n",
    "\n",
    "subjects = {\n",
    "    'STEM': 'high_school_computer_science', \n",
    "    'Humanidades': 'formal_logic',  \n",
    "    'Ci√™ncias Sociais': 'high_school_economics'\n",
    "}\n",
    "\n",
    "SEED = 42 \n",
    "\n",
    "mmlu_suite = {}\n",
    "for category, subject in subjects.items():\n",
    "    filtered_dataset = mmlu_dataset['test'].filter(lambda x: x['subject'] == subject)\n",
    "    \n",
    "    if len(filtered_dataset) < 50:\n",
    "        print(f\"Aviso: Encontradas apenas {len(filtered_dataset)} quest√µes para o assunto '{subject}'. Usando todas as dispon√≠veis.\")\n",
    "        mmlu_suite[category] = filtered_dataset.shuffle(seed=SEED)\n",
    "    else:\n",
    "        mmlu_suite[category] = filtered_dataset.shuffle(seed=SEED).select(range(50))\n",
    "\n",
    "print(f\"Su√≠te MMLU criada com as categorias e n√∫mero de quest√µes:\")\n",
    "for category, data in mmlu_suite.items():\n",
    "    print(f\"- {category}: {len(data)} quest√µes\")\n",
    "def format_mmlu_prompt(question_data, few_shot_examples):\n",
    "    \"\"\"Cria um prompt 4-shot para uma quest√£o do MMLU.\"\"\"\n",
    "    prompt = \"Responda a seguinte quest√£o de m√∫ltipla escolha.\\n\\n\"\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Quest√£o: {ex['question']}\\n\"\n",
    "        prompt += f\"Op√ß√µes: A) {ex['choices'][0]} B) {ex['choices'][1]} C) {ex['choices'][2]} D) {ex['choices'][3]}\\n\"\n",
    "        prompt += f\"Resposta: {['A', 'B', 'C', 'D'][ex['answer']]}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Quest√£o: {question_data['question']}\\n\"\n",
    "    prompt += f\"Op√ß√µes: A) {question_data['choices'][0]} B) {question_data['choices'][1]} C) {question_data['choices'][2]} D) {question_data['choices'][3]}\\n\"\n",
    "    prompt += f\"Resposta:\"\n",
    "    return prompt\n",
    "\n",
    "def evaluate_mmlu(model, tokenizer, suite, dev_split):\n",
    "    \"\"\"Avalia um modelo na su√≠te MMLU usando 4-shot prompting.\"\"\"\n",
    "    results = {}\n",
    "    few_shot_examples = dev_split.select(range(4))\n",
    "    \n",
    "    for category, dataset in suite.items():\n",
    "    \n",
    "        if len(dataset) == 0:\n",
    "            print(f\"Aviso: Nenhuma quest√£o encontrada para a categoria '{category}'. Pulando a avalia√ß√£o.\")\n",
    "            results[category] = 0.0 \n",
    "            continue\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        for item in tqdm(dataset, desc=f\"Avaliando {category}\"):\n",
    "            prompt = format_mmlu_prompt(item, few_shot_examples)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            prediction_text = tokenizer.decode(outputs[0][-5:], skip_special_tokens=True).strip()\n",
    "            predicted_char = prediction_text[0] if prediction_text else ''\n",
    "            \n",
    "            ground_truth_char = ['A', 'B', 'C', 'D'][item['answer']]\n",
    "            \n",
    "            if predicted_char.upper() == ground_truth_char:\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        accuracy = (correct_predictions / len(dataset)) * 100\n",
    "        results[category] = accuracy\n",
    "        \n",
    "    # Filtra os valores None ou N/A antes de calcular a m√©dia\n",
    "    valid_results = [res for res in results.values() if isinstance(res, (int, float))]\n",
    "    if valid_results:\n",
    "        results['Agregada'] = np.mean(valid_results)\n",
    "    else:\n",
    "        results['Agregada'] = 0.0\n",
    "\n",
    "    return results\n",
    "\n",
    "# Avaliar modelo base\n",
    "baseline_mmlu_results = evaluate_mmlu(base_model, tokenizer, mmlu_suite, mmlu_dataset['dev'])\n",
    "\n",
    "# Avaliar modelo fine-tuned\n",
    "finetuned_model_1 = get_finetuned_model(OUTPUT_DIR_CONFIG_1)\n",
    "# finetuned_mmlu_results_1 = evaluate_mmlu(finetuned_model_1, tokenizer, mmlu_suite, mmlu_dataset['dev'])\n",
    "print(\"Fun√ß√£o de avalia√ß√£o MMLU definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 [IMPLEMENTADO] An√°lise de Regress√£o\n",
    "Fun√ß√£o para calcular e exibir a varia√ß√£o percentual de acur√°cia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- An√°lise de Regress√£o de Capacidade (MMLU) ---\n",
      "Acur√°cia Agregada (Base): 77.67%\n",
      "Acur√°cia Agregada (Fine-tuned): 71.00%\n",
      "Varia√ß√£o Percentual Agregada: -8.59%\n",
      "\n",
      "Categoria: STEM\n",
      "  Acur√°cia Base: 75.00% | Acur√°cia Fine-tuned: 70.00% | Varia√ß√£o: -6.67%\n",
      "\n",
      "Categoria: Humanidades\n",
      "  Acur√°cia Base: 80.00% | Acur√°cia Fine-tuned: 72.00% | Varia√ß√£o: -10.00%\n",
      "\n",
      "Categoria: Ci√™ncias Sociais\n",
      "  Acur√°cia Base: 78.00% | Acur√°cia Fine-tuned: 71.00% | Varia√ß√£o: -8.97%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def report_regression_analysis(baseline_results: dict, finetuned_results: dict):\n",
    "    \"\"\"Calcula e imprime a an√°lise de regress√£o de capacidade.\"\"\"\n",
    "    print(\"\\n--- An√°lise de Regress√£o de Capacidade (MMLU) ---\")\n",
    "    \n",
    "    # An√°lise agregada [cite: 43]\n",
    "    agg_baseline = baseline_results['Agregada']\n",
    "    agg_finetuned = finetuned_results['Agregada']\n",
    "    agg_change = ((agg_finetuned - agg_baseline) / agg_baseline) * 100 if agg_baseline > 0 else 0\n",
    "    print(f\"Acur√°cia Agregada (Base): {agg_baseline:.2f}%\")\n",
    "    print(f\"Acur√°cia Agregada (Fine-tuned): {agg_finetuned:.2f}%\")\n",
    "    print(f\"Varia√ß√£o Percentual Agregada: {agg_change:.2f}%\\n\")\n",
    "\n",
    "    # An√°lise por categoria [cite: 43]\n",
    "    for category in subjects.keys():\n",
    "        base = baseline_results[category]\n",
    "        ft = finetuned_results[category]\n",
    "        change = ((ft - base) / base) * 100 if base > 0 else 0\n",
    "        print(f\"Categoria: {category}\")\n",
    "        print(f\"  Acur√°cia Base: {base:.2f}% | Acur√°cia Fine-tuned: {ft:.2f}% | Varia√ß√£o: {change:.2f}%\\n\")\n",
    "\n",
    "# -- Exemplo com dados fict√≠cios. Substitua com seus resultados reais --\n",
    "baseline_results_example = {'STEM': 75.0, 'Humanidades': 80.0, 'Ci√™ncias Sociais': 78.0, 'Agregada': 77.67}\n",
    "finetuned_results_example = {'STEM': 70.0, 'Humanidades': 72.0, 'Ci√™ncias Sociais': 71.0, 'Agregada': 71.0}\n",
    "\n",
    "report_regression_analysis(baseline_results_example, finetuned_results_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
